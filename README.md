# LLM Guard 🔒
LLM Guard is a security framework designed to prevent private data leakage to or from Large Language Models (LLMs). It provides multiple layers of protection to ensure sensitive information is safeguarded when interacting with LLMs.

## Features ✨
- 🔍 **Input Filtering**: Prevents sensitive data from being sent to the LLM.
- 🚨 **Output Monitoring**: Detects and blocks unauthorized data leakage.
- ⚙️ **Customizable Policies**: Define rules to manage and control LLM interactions.
- 📊 **Logging & Auditing**: Tracks interactions for compliance and security analysis.
- 🔗 **Seamless Integration**: Works with various LLM APIs and self-hosted models.

## Installation ⚡
1️⃣ Clone the repository from GitHub.  
2️⃣ Install the required dependencies.  

## Usage 🖥️
- Set up your policies to filter sensitive data.  
- Monitor interactions to prevent unauthorized leakage.  
- Customize rules based on your security needs.  

## Contributing 🤝
We welcome contributions! Feel free to open an issue or submit a pull request to improve LLM Guard.  

## Contact 📬
For queries, reach out via [GitHub Issues](https://github.com/barkhaaroraa/llm_guard_minor/issues).

(winning idea of HackIITK 2025 startup track)

