# LLM Guard ğŸ”’
LLM Guard is a security framework designed to prevent private data leakage to or from Large Language Models (LLMs). It provides multiple layers of protection to ensure sensitive information is safeguarded when interacting with LLMs.

## Features âœ¨
- ğŸ” **Input Filtering**: Prevents sensitive data from being sent to the LLM.
- ğŸš¨ **Output Monitoring**: Detects and blocks unauthorized data leakage.
- âš™ï¸ **Customizable Policies**: Define rules to manage and control LLM interactions.
- ğŸ“Š **Logging & Auditing**: Tracks interactions for compliance and security analysis.
- ğŸ”— **Seamless Integration**: Works with various LLM APIs and self-hosted models.

## Installation âš¡
1ï¸âƒ£ Clone the repository from GitHub.  
2ï¸âƒ£ Install the required dependencies.  

## Usage ğŸ–¥ï¸
- Set up your policies to filter sensitive data.  
- Monitor interactions to prevent unauthorized leakage.  
- Customize rules based on your security needs.  

## Contributing ğŸ¤
We welcome contributions! Feel free to open an issue or submit a pull request to improve LLM Guard.  

## Contact ğŸ“¬
For queries, reach out via [GitHub Issues](https://github.com/barkhaaroraa/llm_guard_minor/issues).

(winning idea of HackIITK 2025 startup track)

